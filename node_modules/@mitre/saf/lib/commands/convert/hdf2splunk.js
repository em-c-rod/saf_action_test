"use strict";
Object.defineProperty(exports, "__esModule", { value: true });
const tslib_1 = require("tslib");
const command_1 = require("@oclif/command");
const hdf_converters_1 = require("@mitre/hdf-converters");
const global_1 = require("../../utils/global");
const fs_1 = tslib_1.__importDefault(require("fs"));
const logging_1 = require("../../utils/logging");
class HDF2Splunk extends command_1.Command {
    async run() {
        const { flags } = this.parse(HDF2Splunk);
        const logger = (0, logging_1.createWinstonLogger)('hdf2splunk', flags.logLevel);
        logger.warn('Please ensure the necessary configuration changes for your Splunk server have been configured to prevent data loss. See https://github.com/mitre/saf/wiki/Splunk-Configuration');
        const inputFile = JSON.parse(fs_1.default.readFileSync(flags.input, 'utf-8'));
        logger.info(`Input File "${(0, global_1.convertFullPathToFilename)(flags.input)}": ${(0, logging_1.getHDFSummary)(inputFile)}`);
        new hdf_converters_1.FromHDFToSplunkMapper(inputFile, logger).toSplunk({
            host: flags.host,
            port: flags.port,
            scheme: flags.scheme,
            username: flags.username,
            password: flags.password,
            index: flags.index,
            insecure: true, // The Splunk SDK's requestOptions somehow broke on release of the mapper, this will be fixed in mitre/heimdall2#2675
        }, (0, global_1.convertFullPathToFilename)(flags.input));
    }
}
exports.default = HDF2Splunk;
HDF2Splunk.usage = 'hdf2splunk -i, --input=FILE -H, --host -P, --port -p, --protocol -t, --token -i, --index';
HDF2Splunk.description = 'Translate and upload a Heimdall Data Format JSON file into a Splunk server via its HTTP Event Collector';
HDF2Splunk.flags = {
    help: command_1.flags.help({ char: 'h' }),
    input: command_1.flags.string({ char: 'i', required: true, description: 'Input HDF file' }),
    host: command_1.flags.string({ char: 'H', required: true, description: 'Splunk Hostname or IP' }),
    port: command_1.flags.integer({ char: 'P', required: false, description: 'Splunk management port (also known as the Universal Forwarder port)', default: 8089 }),
    scheme: command_1.flags.string({ char: 's', required: false, description: 'HTTP Scheme used for communication with splunk', default: 'https', options: ['http', 'https'] }),
    username: command_1.flags.string({ char: 'u', required: true, description: 'Your Splunk username' }),
    password: command_1.flags.string({ char: 'p', required: true, description: 'Your Splunk password' }),
    index: command_1.flags.string({ char: 'I', required: true, description: 'Splunk index to import HDF data into' }),
    logLevel: command_1.flags.string({ char: 'L', required: false, default: 'info', options: ['info', 'warn', 'debug', 'verbose'] }),
};
HDF2Splunk.examples = ['saf convert:hdf2splunk -i rhel7-results.json -H 127.0.0.1 -u admin -p Valid_password! -I hdf'];
